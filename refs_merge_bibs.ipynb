{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_bib_file(filename):\n",
    "    with open(filename) as bibtex_file:\n",
    "        return bibtexparser.load(bibtex_file)\n",
    "\n",
    "def save_bib_file(filename, entries):\n",
    "    db = bibtexparser.bibdatabase.BibDatabase()\n",
    "    db.entries = entries\n",
    "    writer = bibtexparser.bwriter.BibTexWriter()\n",
    "    with open(filename, 'w') as bibtex_file:\n",
    "        bibtex_file.write(writer.write(db))\n",
    "\n",
    "def entry_to_bibtex(entry):\n",
    "    db = bibtexparser.bibdatabase.BibDatabase()\n",
    "    db.entries = [entry]\n",
    "    writer = bibtexparser.bwriter.BibTexWriter()\n",
    "    return writer.write(db)\n",
    "\n",
    "def find_duplicate_entries(bib_database):\n",
    "    id2entries = defaultdict(set)\n",
    "    for entry in bib_database.entries:\n",
    "        id2entries[entry['ID']].add(entry_to_bibtex(entry))\n",
    "    return {ID: entries for ID, entries in id2entries.items() if len(entries) > 1}\n",
    "\n",
    "def check_deduplication_integrity(original_entries, dedup_entries):\n",
    "    missing_ids = set(original_entries.keys()) - set(dedup_entries.keys())\n",
    "    new_ids = set(dedup_entries.keys()) - set(original_entries.keys())\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"Error: The following IDs are missing from the deduplicated set: {missing_ids}\")\n",
    "    else:\n",
    "        print(\"All duplicate element IDs are present in the deduplicated set.\")\n",
    "    \n",
    "    if new_ids:\n",
    "        print(f\"Error: The following IDs are new in the deduplicated set: {new_ids}\")\n",
    "    else:\n",
    "        print(\"All deduplicated IDs are present in the original set.\")\n",
    "\n",
    "    for dedup_id, dedup_entry in dedup_entries.items():\n",
    "        if dedup_id in original_entries and dedup_entry not in original_entries[dedup_id]:\n",
    "            print(f\"Error: Deduplicated entry for ID '{dedup_id}' does not match any original entry.\")\n",
    "\n",
    "    print(\"Integrity check completed.\")\n",
    "\n",
    "def merge_entries(original_db, dedup_db):\n",
    "    merged_entries = []\n",
    "    dedup_ids = set(entry['ID'] for entry in dedup_db.entries)\n",
    "    \n",
    "    for entry in original_db.entries:\n",
    "        if entry['ID'] not in dedup_ids:\n",
    "            merged_entries.append(entry)\n",
    "    \n",
    "    merged_entries.extend(dedup_db.entries)\n",
    "    return merged_entries\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    original_file = 'refs_merged.bib'\n",
    "    duplicate_file = 'duplicate_entries.bib'\n",
    "    deduplicated_file = 'deduplicated_entries.bib'\n",
    "    merged_file = 'refs_merged_dedup.bib'\n",
    "\n",
    "    # Load original database\n",
    "    original_db = load_bib_file(original_file)\n",
    "\n",
    "    # Find and save duplicate entries\n",
    "    duplicate_entries = find_duplicate_entries(original_db)\n",
    "    save_bib_file(duplicate_file, [entry for entries in duplicate_entries.values() for entry in entries])\n",
    "\n",
    "    # Load deduplicated entries (assuming this file has been created by your ML/NLP process)\n",
    "    dedup_db = load_bib_file(deduplicated_file)\n",
    "\n",
    "    # Convert entries to BibTeX format for integrity check\n",
    "    original_entries = {ID: set(entry_to_bibtex(entry) for entry in original_db.entries if ID == entry[\"ID\"]) for ID in duplicate_entries}\n",
    "    dedup_entries = {entry['ID']: entry_to_bibtex(entry) for entry in dedup_db.entries}\n",
    "\n",
    "    # Perform integrity check\n",
    "    check_deduplication_integrity(original_entries, dedup_entries)\n",
    "\n",
    "    # Merge original and deduplicated entries\n",
    "    merged_entries = merge_entries(original_db, dedup_db)\n",
    "\n",
    "    # Save merged entries\n",
    "    save_bib_file(merged_file, merged_entries)\n",
    "\n",
    "    print(f\"Merged entries saved to {merged_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'publisher': 'SIAM', 'year': '1999', 'author': 'Latouche, Guy and Ramaswami, Vaidyanathan', 'title': 'Introduction to matrix analytic methods in stochastic modeling', 'ENTRYTYPE': 'book', 'ID': 'latouche1999introduction'}\n",
      "@book{latouche1999introduction,\n",
      " author = {Latouche, Guy and Ramaswami, Vaidyanathan},\n",
      " publisher = {SIAM},\n",
      " title = {Introduction to matrix analytic methods in stochastic modeling},\n",
      " year = {1999}\n",
      "}\n",
      "\n",
      "{'year': '1999', 'title': 'Introduction to matrix analytic methods in stochastic modeling', 'publisher': 'SIAM', 'author': 'Latouche, Guy and Ramaswami, Vaidyanathan', 'ENTRYTYPE': 'book', 'ID': 'latouche1999introduction'}\n",
      "@book{latouche1999introduction,\n",
      " author = {Latouche, Guy and Ramaswami, Vaidyanathan},\n",
      " publisher = {SIAM},\n",
      " title = {Introduction to matrix analytic methods in stochastic modeling},\n",
      " year = {1999}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type control not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': '1999', 'title': 'Introduction to matrix analytic methods in stochastic modeling', 'publisher': 'SIAM', 'author': 'Latouche, Guy and Ramaswami, Vaidyanathan', 'ENTRYTYPE': 'book', 'ID': 'latouche1999introduction'}\n",
      "@book{latouche1999introduction,\n",
      " author = {Latouche, Guy and Ramaswami, Vaidyanathan},\n",
      " publisher = {SIAM},\n",
      " title = {Introduction to matrix analytic methods in stochastic modeling},\n",
      " year = {1999}\n",
      "}\n",
      "\n",
      "{'publisher': 'Taylor \\\\& Francis', 'year': '2010', 'pages': '324--335', 'number': '489', 'volume': '105', 'journal': 'Journal of the American Statistical Association', 'author': 'Braun, Michael and McAuliffe, Jon', 'title': 'Variational inference for large-scale models of discrete choice', 'ENTRYTYPE': 'article', 'ID': 'braun2010variational'}\n",
      "@article{braun2010variational,\n",
      " author = {Braun, Michael and McAuliffe, Jon},\n",
      " journal = {Journal of the American Statistical Association},\n",
      " number = {489},\n",
      " pages = {324--335},\n",
      " publisher = {Taylor \\& Francis},\n",
      " title = {Variational inference for large-scale models of discrete choice},\n",
      " volume = {105},\n",
      " year = {2010}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type control not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': '2010', 'volume': '105', 'title': 'Variational inference for large-scale models of discrete choice', 'publisher': 'Taylor \\\\& Francis', 'pages': '324--335', 'number': '489', 'journal': 'Journal of the American Statistical Association', 'author': 'Braun, Michael and McAuliffe, Jon', 'ENTRYTYPE': 'article', 'ID': 'braun2010variational'}\n",
      "@article{braun2010variational,\n",
      " author = {Braun, Michael and McAuliffe, Jon},\n",
      " journal = {Journal of the American Statistical Association},\n",
      " number = {489},\n",
      " pages = {324--335},\n",
      " publisher = {Taylor \\& Francis},\n",
      " title = {Variational inference for large-scale models of discrete choice},\n",
      " volume = {105},\n",
      " year = {2010}\n",
      "}\n",
      "\n",
      "{'year': '2010', 'volume': '105', 'title': 'Variational inference for large-scale models of discrete choice', 'publisher': 'Taylor \\\\& Francis', 'pages': '324--335', 'number': '489', 'journal': 'Journal of the American Statistical Association', 'author': 'Braun, Michael and McAuliffe, Jon', 'ENTRYTYPE': 'article', 'ID': 'braun2010variational'}\n",
      "@article{braun2010variational,\n",
      " author = {Braun, Michael and McAuliffe, Jon},\n",
      " journal = {Journal of the American Statistical Association},\n",
      " number = {489},\n",
      " pages = {324--335},\n",
      " publisher = {Taylor \\& Francis},\n",
      " title = {Variational inference for large-scale models of discrete choice},\n",
      " volume = {105},\n",
      " year = {2010}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type control not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': '2019', 'booktitle': 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics', 'author': 'Hazra, Sayan and Banerjee, Soumi and Ghosh, Kripabandhu and Ghosh, Saptarshi and Mehta, Parth', 'title': 'A DET for Natural Language Inference: Generating Natural Language Inference Explanation Trees using Deep Generative Models', 'ENTRYTYPE': 'inproceedings', 'ID': 'hazra2019det'}\n",
      "@inproceedings{hazra2019det,\n",
      " author = {Hazra, Sayan and Banerjee, Soumi and Ghosh, Kripabandhu and Ghosh, Saptarshi and Mehta, Parth},\n",
      " booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n",
      " title = {A DET for Natural Language Inference: Generating Natural Language Inference Explanation Trees using Deep Generative Models},\n",
      " year = {2019}\n",
      "}\n",
      "\n",
      "{'year': '2019', 'title': 'A DET for Natural Language Inference: Generating Natural Language Inference Explanation Trees using Deep Generative Models', 'booktitle': 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics', 'author': 'Hazra, Sayan and Banerjee, Soumi and Ghosh, Kripabandhu and Ghosh, Saptarshi and Mehta, Parth', 'ENTRYTYPE': 'inproceedings', 'ID': 'hazra2019det'}\n",
      "@inproceedings{hazra2019det,\n",
      " author = {Hazra, Sayan and Banerjee, Soumi and Ghosh, Kripabandhu and Ghosh, Saptarshi and Mehta, Parth},\n",
      " booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n",
      " title = {A DET for Natural Language Inference: Generating Natural Language Inference Explanation Trees using Deep Generative Models},\n",
      " year = {2019}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "\n",
    "# List of BibTeX IDs to search for\n",
    "bibtex_ids = [\n",
    "   \"latouche1999introduction\", \"braun2010variational\",\"hazra2019det\" \n",
    "]\n",
    "\n",
    "def search_bib_files():\n",
    "    for ID in bibtex_ids:\n",
    "        for filename in os.listdir('.'):\n",
    "            if '.bib' in filename:\n",
    "                with open(filename, 'r', encoding='utf-8') as bibtex_file:\n",
    "                    bib_database = bibtexparser.load(bibtex_file)\n",
    "                    \n",
    "                    for entry in bib_database.entries:\n",
    "                        if entry['ID'] == ID:\n",
    "                            print(entry)\n",
    "                            print(entry_to_bibtex(entry))\n",
    "                            # print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_bib_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
