\chapter{Obtaining isometry with normalization}\label{ch:isometry_normalization}

Normalization layers, such as batch normalization~\citep{ioffe2015batch} and layer normalization~\citep{ba2016layer}, are essential components of neural architecture design. They have been shown to improve the training stability and speed of deep neural networks~\citep{he2016deep, devlin2018bert}. In this chapter, we explore the isometry properties of normalization layers. While in Chapter~\ref{ch:bn_ortho}, we focused on the orthogonality properties of normalization layers, here we investigate the isometry properties of these layers, which will be formalized later. This chapter is dedicated to one of the primary results of this thesis, which is the isometry bias of normalization layers. While conceptually orthogonal and isometry properties are related, the striking property of isometry is that it holds deterministically for all matrices, while orthogonality properties discussed in Chapter~\ref{ch:bn_ortho} hold in expectation, with respect to a particular initialization of the weights. 


\section{Gram matrices and isometry}
Given $n$ data points $\{x_i\}_{i\le n}\in \R^d$, the Gram matrix $G^\ell$ of the feature vectors $x_1^\ell,\dots,x_n^\ell \in\R^d$ at layer $\ell$ of the network is defined as
\begin{align}  \label{iso:eq:gram_matrix}
G^\ell := \left[\ip{x^\ell_i}{x_j^\ell}\right]_{i,j\le n}  , && \ell=1,\dots,L.
\end{align}

Intuitively, an isometric Gram matrix implies that the network preserves the distances and angles between the input data points after mapping them to the feature space. 
Isometry of Gram matrices can be quantified using the eigenvalues of $G^\ell.$  
One possible way to formulate isometry is to use the ratio of the volume and scale of the parallelepiped spanned by the feature vectors $x_1^\ell,  \dots,x^\ell_n$. 
For example, consider two points on a plane $x_1, x_2 \in \mathbb{R}^2$ with lengths $a = |x_1|, b = |x_2|$ and angle $\theta = \angle(x_1, x_2)$. The ratio is given by $a b \sin(\theta) / (a^2 + b^2)$, which is maximized when $a=b$ and $\theta = \pi/2$. This is shown for $n=2$ and $n=3$ feature vectors in Figure~\ref{iso:fig:isometry}. 

\begin{figure}[ht]
    \centering
    \input{illustrations/isometry_idea.tikz}
    \caption{A geometric interpretation of isometry: higher volume corresponds to higher isometry.}
    \label{iso:fig:isometry}
\end{figure}

Inspired by this intuition, we can define the isometry of the Gram matrix. 

\begin{definition}
Let $M$ be an $n \times n$ positive semi-definite matrix. We define the \emph{isometry $\Iso(M)$} of $M$ as the ratio of its normalized determinant to its normalized trace:
\begin{align}\label{iso:eq:isometry}
\Iso(M) := \frac{\det(M)^{1/n}}{\frac1n\tr(M)}.
\end{align}
\end{definition}
The function $\Iso(M)$ defined in \eqref{iso:eq:isometry} quantifies how well $M$ approximates the identity matrix $I_n$. We can easily check that $\Iso(M)$ has some desirable properties (see Lemma~\ref{iso:lem:isometry_basic} for formal statements and proofs):
\begin{itemize}
    \item Scale-invariance: Multiplying $M$ by a constant does not affect $\Iso(M)$.%: $\Iso(c M) = \Iso(M)$
    \item Isometry-preserving: $\Iso(M)$ ranges between $0$ and $1$, with $0$ and $1$ corresponding to degenerate and identity matrices respectively.
    \item Isometry gap: $\ID(M)$ lies between $0$ and $\infty,$ with $0$ and $\infty$ indicating identity and degenerate matrices respectively.
\end{itemize}
These properties suggest that $\Iso(M)$ is a suitable function for measuring how close a matrix is to being an isometry, i.e., a transformation that preserves distances between metric spaces. Moreover, there is a clear link between isometry and normalization, which we will explore in the next section. We will often measure how far a matrix is from isometry by its negative log $\ID(M),$ which we will call \emph{isometry gap}.

\paragraph{Basic properties of isometry}
It is straightforward to check isometry obeys the following basic isometry-preserving properties: 
\begin{lemma}\label{iso:lem:isometry_basic}
For PSD matrix $M,$ the isometry defined in~\eqref{iso:eq:isometry} obeys the following properties: 1) scale-invariance $\Iso(c M) = \Iso(M),$ 2) only takes value in the unit range $\Iso(M)\in [0,1]$ 3) it takes its maximum value if and only if $M$ is identity $\Iso(M)=1\iff M=I_n,$ and 3) takes minimum value if and only if $M$ is degenerate $\Iso(M)=0. $
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{iso:lem:isometry_basic}]
 The scale-invariance is trivially true as scaling $M$ by any constant will scale $\det(M)^{1/n}$ and $\tr(M)$ by the same amount. The proof of other properties is a straightforward consequence of writing the isometry in terms of the eigenvalues $\Iso(M) = (\prod_i \lambda_i)^{1/n}/(\frac1n\sum_i \lambda_i ), $ where $\lambda_i$'s are eigenvalues of $M.$ By arithmetic vs geometric mean inequality over the eigenvalues we have $(\prod_i \lambda_i)^{1/n}\le \frac1n\sum_i \lambda_i ),$ which proves that $\Iso(M)\in [0,1].$ Furthermore, the inequality is tight if and only if the values are all equal $\lambda_1 = \dots =\lambda_n,$ which holds only for an identity $M=I_n$. Finally, the isometry is zero if and only if at least one eigenvalue is zero, which is the case for degenerate matrix $M.$ 
\end{proof}


\section{Isometry bias of normalization}

This notion of isometry has a remarkable property: if we normalize each point by its Euclidean norm  then the isometry of their associated Gram matrix does not decrease. We formalize this property in the following theorem.


\begin{theorem}\label{iso:thm:isometry_normalization}
Given $n$ samples $\{x_i\}_{i\le n}\subset \R^d\setminus\{\0_d\}$, and their projection onto the unit sphere $\Norm{x}_i:=x_i/\norm{x_i},$ and their respective Gram matrices $G = \left[\ip{x_i}{x_j}\right]_{i,j\le n}$ and $\Norm{G} = \left[\ip{\Norm{x_i}}{\Norm{x_j}}\right]_{i,j\le n}.$ The isometry of Gram matrices obeys
\begin{align}
 \Iso\left(\Norm{G}\right) \ge \Iso(G)\left(1+\frac{\frac1n \sum_i^n(a_i-\bar{a})^2}{\bar{a}^2}\right), && \text{where } a_i:=\norm{x_i}, \bar{a}:=\frac1n\sum_i^n a_i. 
\end{align}
\end{theorem}

Theorem~\ref{iso:thm:isometry_normalization} shows a subtle property of normalization: Because the terms $(a_i-\Bar{a})^2$ are always non-negative, the left-hand side is always greater than or equal to $\Iso(G).$ It further quantifies the improvement in isometry as a function of variation of norms. The terms $\Bar{a}$ and $\frac1n \sum_i^n(a_i-\bar{a})^2$ correspond to the sample mean and variance of $a_1,\dots,a_n.$ 
Thus, the more diverse the norms, the larger the improvement in isometry. 
% Note the isometry does not increase if and only if the norms are all equal $a_1=\dots=a_n.$

% \section{Proof of Theorem~\ref{iso:thm:isometry_normalization} and properties of isometry}\label{iso:sec:isometry_theorems}


% \subsection{Proof of Thm~\ref{iso:thm:isometry_normalization} }

\begin{proof}[\textbf{Proof of Theorem~\ref{iso:thm:isometry_normalization}}] 
Define $D :=\diag(a_1/\sqrt{d} ,\dots, a_n/\sqrt{d}).$ Observe that $C = D \Norm{G} D,$ implying $ \det(G) = \det(\Norm{G}) \det(D)^{2}.$ Because $\Norm{x_i}$'s have norm $\sqrt{d},$ diagonals of  Gram after normalization are constant $\Norm{G}_{ii}=d,$ implying $\frac1n\tr(\Norm{G})= d.$ We have
\begin{align}
\frac{\Iso(\Norm{G})}{\Iso(G)}
&=
\frac{\frac1n\tr(G)}{\frac1n\tr(\Norm{G})}\frac{\det(\Norm{G})^{1/n}}{\det(G)^{1/n}}\\
&= \frac{\frac1n\sum_i^n a_i^2}{d}\frac{\det(\Norm{G})^{1/n}}{\det(\Norm{G})^{1/n}(d^{-n} \prod_i^n a_i^2)^{1/n}}\\
&= \frac{(\frac1n\sum_i a_i)^2}{(\prod_i^n a_i)^{2/n}}\frac{\frac1d\sum_i^n a_i^2}{(\frac1n\sum_i a_i)^2}\\
% & \geq  \frac{\frac1n\sum_i^n a_i^2}{(\frac1n\sum_i^n a_i)^2}\\
&= 1+\frac{\frac1n \sum_i^n(a_i-\bar{a})^2}{\bar{a}^2}, && \bar{a}:=\frac1n\sum_i^n a_i
\end{align}
\end{proof}




\section{Implications for normalization layers}
Theorem~\ref{iso:thm:isometry_normalization} allows us to highlight the isometry bias of layer and batch normalization. 


\begin{corollary} \label{iso:cor:ln}
Consider $n$ vectors before and after layer-normalization $\{x_i\}_{i\le n}\subset \R^d\setminus\{\0_d\}$ and $\{\Norm{x}_i\}_{i\le n}, \Norm{x_i}:=\ln{x_i}.$
Define their respective Gram matrices $G := [\ip{x_i}{x_j}]_{i,j\le n},$ and $\Norm{G}:=[\ip{\Norm{x}_i}{\Norm{x}_j}]_{i,j\le n}.$ We have:
\begin{align*}
 \Iso\left(\Norm{G}\right) \ge \Iso(G)\left(1+\frac{\frac1n \sum_i^n(a_i-\bar{a})^2}{\bar{a}^2}\right), && \text{where } a_i:=\norm{x_i},\bar{a}:=\frac1n\sum_i^n a_i. 
\end{align*}
% with equality holding if and only if columns have equal norms $a_1=\dots=a_n$.
\end{corollary}

Observe that we can view the layer normalization as a projection onto the $\sqrt{d}$-sphere, which is equivalent to the unit-norm projection in Theorem~\ref{iso:thm:isometry_normalization} up to a constant scale factor. Since isometry is scale-invariant, this implies that corollary~\ref{iso:cor:ln} follows directly from Theorem~\ref{iso:thm:isometry_normalization}. 

Moreover, corollary~\ref{iso:cor:ln} shows that the isometry of layer normalization is deterministic and does not rely on random weights. This means that layer normalization always preserves or enhances the isometry of representations, even during training. We provide empirical evidence for this in discussion.

Despite the seemingly vast differences between layer normalization and batch normalization, the following corollary shows an intimate link between them through the prism of isometry. 

\begin{corollary} \label{iso:cor:bn}
 Given $n$ samples in a mini-batch before $X\in\R^{d\times n},$ and after normalization $\Norm{X}=\BN{X}$
 and define covariance matrices $C := X X^\top $ and $\Norm{C}:=X X^\top. $ We have:
\begin{align*}
 \Iso\left(\Norm{C}\right) \ge \Iso(C)\left(1+\frac{\frac1d \sum_i^d(a_i-\bar{a})^2}{\bar{a}^2}\right), && \text{where } a_i:=\norm{X_{i\cdot}},\bar{a}:=\frac1n\sum_{i=1}^d a_i. 
\end{align*}
\end{corollary}

Gram matrices of networks with batch normalization have been the subject of many previous studies at network initialization: it has been postulated that BN prevents rank collapse issue~\citep{daneshmand2020batch} and that it orthogonalizes the representations~\citep{daneshmand2021batch}, and that it imposes isometry~\citep{yang2018a}. The isometry results implied by Corollaries~\ref{iso:cor:bn} give a geometric interpretation of all these findings: it is straightforward to verify that maximum isometry is achieved with identity Gram matrix, which implies orthogonalization of these results. Rather strikingly, while all the results stated before have been of a probabilistic nature, the improved isometry of the Gram matrix implied by Corollary~\ref{iso:cor:bn} holds deterministically.
